<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <title>Safety Systems | Open AI</title>
  <link rel="stylesheet" href="research.css">
</head>
<body>
  <section>
    <div class="container">
      <nav>
        <ul class="unordered">
          <svg class=" color cha w-[1.60625rem] m:w-[1.375rem] h-auto" width="56" viewBox="0 0 28 28" fill="none"
            xmlns="http://www.w3.org/2000/svg">
            <path
            d="M26.153 11.46a6.888 6.888 0 0 0-.608-5.73 7.117 7.117 0 0 0-3.29-2.93 7.238 7.238 0 0 0-4.41-.454 7.065 7.065 0 0 0-2.41-1.742A7.15 7.15 0 0 0 12.514 0a7.216 7.216 0 0 0-4.217 1.346 7.061 7.061 0 0 0-2.603 3.539 7.12 7.12 0 0 0-2.734 1.188A7.012 7.012 0 00 .966 8.268a6.979 6.979 0 0 0 .88 8.273 6.89 6.89 0 0 0 .607 5.729 7.117 7.117 0 0 0 3.29 2.93 7.238 7.238 0 0 0 4.41.454 7.061 7.061 0 0 0 2.409 1.742c.92.404 1.916.61 2.923.604a7.215 7.215 0 0 0 4.22-1.345 7.06 7.06 0 0 0 2.605-3.543 7.116 7.116 0 0 0 2.73 4-1.187 7.01 7.01 0 0 0 1.993-2.196 6.978 6.978 0 0 0-.884-8.27Zm-10.61 14.71c-1.412 0-2.505-.428-3.46-1.215.043-.023.119-.064.168-.094l5.65-3.22a.911.911 0 0 0 .464-.793v-7.86l2.389 1.36a.087.087 0 0 1 .046.065v6.508c0 2.952-2.491 5.248-5.257 5.248ZM4.062 21.354a5.17 5.17 0 0 1-.635-3.516c.042.025.115.07.168.1l5.65 3.22a.928.928 0 0 0 .928 0l6.898-3.93v2.72a.083.083 0 0 1-.034.072l-5.711 3.255a5.386 5.386 0 0 1-4.035.522 5.315 5.315 0 0 1-3.23-2.443ZM2.573 9.184a5.283 5.283 0 0 1 2.768-2.301V13.515a.895.895 0 0 0 .464.793l6.897 3.93-2.388 1.36a.087.087 0 0 1-.08.008L4.52 16.349a5.262 5.262 0 0 1-2.475-3.185 5.192 5.192 0 0 1 .527-3.98Zm19.623 4.506-6.898-3.93 2.388-1.36a.087.087 0 0 1 .08-.008l5.713 3.255a5.28 5.28 0 0 1 2.054 2.118 5.19 5.19 0 0 1-.488 5.608 5.314 5.314 0 0 1-2.391.742v-6.633a.896.896 0 0 0-.459-.792Zm2.377-3.533a7.973 7.973 0 0 0-.168-.099l-5.65-3.22a.93.93 0 0 0-.928 0l-6.898 3.93V8.046a.083.0 83 0 0 1 .034-.072l5.712-3.251a5.375 5.375 0 0 1 5.698.241 5.262 5.262 0 0 1 1.865 2.28c.39.92.506 1.93.335 2.913ZM9.631 15.009l-2.39-1.36a.083.083 0 0 1-.046-.065V7.075c.001-.997.29-1.973.832-2.814a5.297 5.297 0 0 1 2.231-1.935 5.382 5.382 0 0 1 5.659.72 4.89 4.89 0 0 0-.168.093l-5.65 3.22a.913.913 0 0 0-.465.793l-.003 7.857Zm1.297-2.76L14 10.5l3.072 1.75v3.5L14 17.499l-3.072-1.75v-3.5Z"
            fill="currentColor"></path>
        </svg>
        <li class="dropdown">
          <button class="dropbtn">Research</button>
            <div class="dropdown-content">
              <a href="Research Overview.html">Overview</a>
          </div>
        </li> 
        <li class="dropdown">
          <button class="dropbtn">Products</button>
            <div class="dropdown-content">
              <a href="Products.html"> Platform Overview</a>
              <a href="business.html">OpenAI for Business</a>
          </div>
        </li>
        <li class="dropdown">
          <button class="dropbtn">Safety</button>
            <div class="dropdown-content">
              <a href="Safety Overview.html">Safety Overview</a>
              <a href="Safety Standards.html">Safety Standards</a>
              <a href="Safety Systems.html">Safety Systems</a>
          </div>
        </li>
        <li class="dropdown">
          <button class="dropbtn">Company</button>
            <div class="dropdown-content">
              <a href="OpenAI Charter.html">Our Charter</a>
              <a href="Residency.html">Residency</a>
              <a href="Security.html">Security and Privacy</a>
          </div>
        </li>
        <svg class="color sear" xmlns="http://www.w3.org/2000/svg" x="0px" y="0px" width="20" height="20"
            viewBox="0 0 50 50">
            <path
              d="M 21 3 C 11.621094 3 4 10.621094 4 20 C 4 29.378906 11.621094 37 21 37 C 24.710938 37 28.140625 35.804688 30.9375 33.78125 L 44.0937 5 46.90625 L 46.90625 44.09375 L 33.90625 31.0625 C 36.460938 28.085938 38 24.222656 38 20 C 38 10.621094 30.378906 3 21 3 Z M 21 5 C 29.296875 5 36 11.703125 36 20 C 36 28.296875 29.296875 35 21 35 C 12.703125 35 6 28.296875 6 20 C 6 11.703125 12.703125 5 21 5 Z">
            </path>
          </svg>
        </ul>
      </nav>
      <center>
        <p class="chat">Safety Systems
        </p>
      </center>
      <center>
        <p > The Safety Systems team is dedicated to ensuring <br> the safety, robustness, and reliability of AI <br> models and their deployment in the real world.
        </p>
      </center>
      <div class="container">
        <div class="wrap">
            <p class="omg">Building on the many years of our practical alignment work and applied safety efforts, <br> Safety Systems addresses emerging safety issues and develops new fundamental <br> solutions to enable the safe deployment of our most advanced models and future AGI, to <br> make AI that is beneficial and trustworthy.</p>
    <p class="omg">Safety Systems stays closest to deployment risks while our Superalignment team focuses <br> on aligning superintelligence and our <a href="">Preparedness</a> team focuses on safety assessments <br> for frontier models. In collaboration, these teams span a wide spectrum of technical efforts <br> tackling AI safety challenges at OpenAI.</p>
    <br>
            </div>
            </div>
        <div class="container">
            <div class="wrap">
                <p class="ov">Our approach</p>
                <p class="omg">We believe that safe AGI cannot be developed in a vacuum. Learning how to safely deploy <br> powerful models and future AGI requires consistent learning, iterative practice, and <br> research in the real world. We continue to invest in model behavior alignment, safety & <br> ethical reasoning skills in foundation models, end-to-end safety infrastructure, as well as <br> human values alignment via human-AI collaboration on policy development.</p>
                <p class="ov">Problems</p>
                <p class="omg">Safe deployment of AI models requires solving a new and evolving set of technical <br> challenges and open-ended safety problems. Some examples right now are:</p>
            </div>
        </div>
                <ul>
                    <li>How do we ensure our models robustly avoid giving unsafe or inappropriate answers, <br> while also still giving useful and trustworthy answers in a wide range of applications, from <br> high-stakes domains to playful applications?</li>
                    <li>How do we detect unknown classes of harmful answers, actions or usage?</li>
                    <li>How do we maintain user privacy while ensuring safety?</li>
                    <li>How do we build AI to be collaborative with users and safely take actions on behalf of <br> those users?</li>
                    <li>How can we use the model to red-team another model to discover novel failure cases?</li>
                    <li>How do we best leverage diverse human expertise to guide AI safety?</li>
                    <li>How do we share our learnings and solutions to uplift safety across the industry?</li>
                </ul>      
        <br><br></br><br></br>
        <center>
        <img width="1080"  height="1024" src="https://images.ctfassets.net/kftzwdyauwt9/22d2c960-a29d-4cfc-13c33a27dc99/6fb1740608351bf947899822d89226b3/stangel-2022-0558.jpg?w=1920&q=90&fm=webp">
    </center>      
    <br><br><br><br>
    <div class="container">
        <div class="wrap">
            <p class="ov">Lessons</p>
            <p class="omg">We need to approach AI safety from first principles, using AI itself to solve AI safety <br> challenges and building general solutions for categories of problems. </p>
            <p class="omg">There is a delicate tradeoff between safe behavior and usefulness of the model. For <br> example, as the team builds reliable and robust refusal behavior into the model, it is crucial <br> to draw the right boundary and understand the context in order to prevent over-refusal <br> scenarios.</p>
         <p class="omg">Solid engineering work and infrastructure are the foundation. It enables fast iterations of <br> research and various mitigations via analyzing real-world data and use cases, fast <br> prototyping and smooth deployment. We are designing and building a safety service <br> centered around model capability for automated investigation, analysis, enforcement <br> decisioning and a better data flywheel back into model training.</p>
         <p class="ov">Team</p>
         <p class="omg">Safety Systems brings together a diverse team of experts in engineering, research, policy, <br> human-AI collaboration, and product management. This combination of talents has proven <br> to be highly effective, enabling us to access a wide spectrum of solutions ranging from pre- <br> training improvement and model fine-tuning to inference-time monitoring and mitigation.</p>
            <p class="omg">Safety Systems consists of four subteams.</p>
        </div>
    </div>
        <ul>
            <li>Safety Engineering: The team implements system level mitigation into products, builds a <br> secure, privacy-aware, centralized safety service infra, and creates ML-centric toolings <br> for investigation and enforcement at scale.</li>
            <li>Model Safety Research: Model behavior alignment is a core focus of our work, with the <br> goal of creating safer models that behave in alignment with our values and are reliable <br> and controllable. The team advances our capabilities for precisely implementing robust, <br> safe behavior in our models.</li>
            <li>Safety Reasoning Research: Detecting and understanding risks, both knowns and <br> unknowns, is essential to guide the design of default safe model behavior and <br> mitigations. The team is working towards this goal by building better safety and ethical <br> reasoning skills into the foundation model and using these skills to enhance our <br> moderation(opens in a new window) models.</li>
            <li>Human-AI Interaction: Policy is the interface for aligning model behavior with desired <br> human values and we co-design policy with models and for models, and thus policies can <br> be directly plugged into our safety systems. Human experts also provide feedback for the <br> system for alignment with human expectations in various stages.</li>
        </ul> 
<br><br><br><br></br>
<center>
    <img width="1080"  height="800" src="https://images.ctfassets.net/kftzwdyauwt9/34802e94-9106-47d1-1ef221387e48/03d90c9869c77227e954589e82c7dda2/stangel-2022-0594.jpg?w=1920&q=90&fm=webp">
</center>
    <br><br><br><br>
    <div class="container">
        <div class="wrap">
        <p class="ov"> Join us</p>
        <p class="omg">Safety Systems is at the forefront of AI safety research and development. If you're <br> interested in being part of this groundbreaking work, we invite you to apply for our safety <br> engineer and research engineer positions. Come work on making AI systems safe and <br> beneficial for humanity.</p>
   <a href=""><button type="button" class="systems">&nbsp; View careers
    <path d="M1 9L9 1M9 1H2.5M9 1V7.22222" stroke="currentColor" stroke-width="1.25" stroke-linecap="round"
      stroke-linejoin="round"></path>
  </svg></button>
</a>
</div>
</div>
<br><br><br>
<br><br><br>
</section>
</body>
</html>